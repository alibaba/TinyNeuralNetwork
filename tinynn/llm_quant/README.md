# LLM QUANT

## 安装依赖

- PyTorch: tested on PyTorch 1.13 & CUDA 11.6
- transformers: tested on v4.28.1
- easyquant: 需要到[Releases](https://github.com/alibaba/TinyNeuralNetwork/releases)手动下载安装包进行安装, 提供权重动态解压和动态量化的cuda加速kernel

## 量化模式

- 8bit仅权重量化: 权重压缩为8-bit,显存需求降低,计算时还原为FP16进行计算，相比于FP16的模型推理存在额外开销。模型精度几乎没有影响。
- 4bit仅权重量化: 权重压缩为4-bit,显存需求大幅度降低, 计算时还原为FP16进行计算，相比于FP16的模型推理存在额外开销。模型精度下降较严重。
- token-wise动态量化: 权重压缩为8-bit, 激活值运行时动态量化为8-bit, 结合easyquant库的int8 GEMM可以有效提升推理性能。在Llama-family模型中精度小幅度下降，基本没有影响。

## Llama 量化
我们对llama模型进行了详细的量化分析和测试，推荐使用8-bit的动态量化，其可以有效提升推理速度并降低显存需求，同时精度几乎不受影响。

| 量化模式                    | wikitext2(ppl⬇️) | 推理性能(ms/token) <br/>GPU:2080Ti | 推理性能(ms/token)<br/> GPU:T4 | 模型占用显存(GB) |
|-------------------------|------------------|--------------------------------|----------------------------|------------|
| llama-7b fp16           | 5.68             | -                              | 61.5882                    | 12.90      |
| llama-7b weight8        | 5.68             | 68.6845                        | 151.1209                   | 7.10       |
| llama-7b token-wise动态量化 | 5.82(+0.14)      | 43.0228                        | 47.1649                    | 7.09       |
| llama-7b weight4        | 6.5657(+0.89)    | 63.7035                        | 141.1330                   | 3.99       |

> 除了模型占用显存外，在模型推理过程中还存在激活值和上下文的显存占用，需要预留1~2GB的额外显存。

## 未来工作

- 4-bit量化精度恢复及加速推理
- 8-bit静态量化
